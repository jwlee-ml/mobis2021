{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"1.TensorFlow_2_x_Basics.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"spMfOdESwP16"},"source":["# TensorFlow 2.x Basics\n","\n","TensorFlow는 ML 모델을 개발하고 학습시키는 데 도움이 되는 핵심 오픈소스 라이브러리입니다.\n","\n","TensorFlow와 Keras는 모두 약 4년전쯤 릴리즈 되었습니다 (Keras는 2015년 3월, TensorFlow는 2015년 11월). 이는 딥러닝 세계의 관점에서 볼 때, 꽤 오랜시간이라고 볼 수 있습니다!\n","\n","과거에 TensorFlow 1.x + Keras는 여러가지 알려진 문제점을 가지고 있었습니다:\n","- TensorFlow를 사용한다는것은 정적인 계산 그래프를 조작함을 의미하는것으로, Imperative 코딩 스타일을 사용하는 프로그래머로 하여금 어렵고, 불편한 느낌을 받게 했었습니다.\n","- TensorFlow API가 매우 강력하면서도 유연하지만, 빠른 코드의 작성의 가능성이 결여되어 있었으며 종종 사용법은 어렵고 혼란스러웠습니다.\n","- Keras는 매우 생산적이고 사용이 쉽지만, 연구에 사용된 사례에서 종종 유연성이 결여되었었습니다.\n","\n","### TensorFlow 2.0은 TensorFlow와 Keras를 대대적으로 새로이 디자인한 것으로, 지난 4년간의 사용자 피드백과 기술의 진보가 모두 고려되었습니다. 위에서 언급된 문제점들을 대규모로 수정합니다.\n","\n","TensorFlow 2.0은 아래와 같은 주요 아이디어에 기반하고 있습니다:\n","\n","- 사용자들이 계산을 eager mode로 수행할 수 있게 해줍니다. 이는 Numpy를 사용하는법과 유사하며, TensorFlow 2.0을 이용한 프로그래밍이 직관적이며 동시에 pythonic할 수 있게끔 해 줍니다.\n","- 1.x에서의 컴파일된 그래프의 엄청난 이점을 그대로 보존하는데, 이는 성능, 분산, 그리고 배포를 위함입니다. 이 내용은 TensorFlow를 빠르고, 분산 구조에서의 확장 가능하며, 상용화에 준비될 수 있도록 해 줍니다.\n","- Keras를 딥러닝의 고수준 API로 채택하여, TensorFlow를 이해하기 쉬우면서도 높은 생산성을 가질 수 있게 만들어 줍니다.\n","- 매우 고수준(더 쉬운 사용성, 약간 부족한 유연성) 에서부터 매우 저수준(더 깊은 전문성, 매우 뛰어난 유연성)의 다양한 범위의 작업으로까지 Keras를 확장합니다."]},{"cell_type":"code","metadata":{"id":"imjVXjMAwP2A"},"source":["## TensorFlow library는 아래와 같이 import 합니다\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4XOhVMSUwP2E"},"source":["## TensorFlow와 함께 많이 쓰이는 numpy library는 아래와 같이 import 합니다\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8tUwsb4wP2H"},"source":["## TensorFlow 내부에는 TensorFlow를 쉽고 편하게 쓸 수 있게 해주는 high-level framework인 Keras가 포함되어 있습니다.\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ht5ijiawP2K"},"source":["## TensorFlow와 Keras의 version을 확인해봅시다\n","print(tf.__version__)\n","print(keras.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-e4IpPvMC5zo"},"source":["## TensorFlow 2.0 시작하기"]},{"cell_type":"markdown","metadata":{"id":"7NAbSZiaoJ4z"},"source":["[MNIST 데이터셋](http://yann.lecun.com/exdb/mnist/)을 로드하여 준비합니다. 샘플 값을 정수에서 부동소수로 변환합니다:"]},{"cell_type":"code","metadata":{"id":"1d9tydUBC2tq"},"source":["mnist = keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPZ68wASog_I"},"source":["층을 차례대로 쌓아 `tf.keras.Sequential` 모델을 만듭니다. 훈련에 사용할 옵티마이저(optimizer)와 손실 함수를 선택합니다:"]},{"cell_type":"code","metadata":{"id":"h3IKyzTCDNGo"},"source":["model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ix4mEL65on-w"},"source":["모델을 훈련하고 평가합니다:"]},{"cell_type":"code","metadata":{"id":"F7dTAzgHDUh7"},"source":["model.fit(x_train, y_train, epochs=10)\n","\n","model.evaluate(x_test,  y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQq8nPY0Hd0u"},"source":["데이터를 탐색해봅시다:"]},{"cell_type":"code","metadata":{"id":"rWbnL92ZC2zG"},"source":["idx = np.random.randint(len(x_train))\n","image = x_train[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8OyIg4GC28H"},"source":["import matplotlib.pyplot as plt\n","plt.imshow(image, cmap='gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQ-dwddzqo3o"},"source":["y_train[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwBxdGyAKnOf"},"source":["내가 쓴 손글씨로 Test 해봅시다"]},{"cell_type":"code","metadata":{"id":"HSeObDSFC23p"},"source":["import os\n","from PIL import Image\n","## 그림판을 이용하여 손으로 숫자를 쓴 다음 파일로 저장하고 아래를 실행하여 upload 합니다\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RS-r-MSTJKF7"},"source":["## image file의 경로 설정\n","cur_dir = os.getcwd()\n","img_path = os.path.join(cur_dir, 'image.png')\n","## image file 읽기\n","cur_img = Image.open(img_path)\n","## 28x28로 resize\n","cur_img = cur_img.resize((28, 28))\n","image = np.asarray(cur_img)\n","\n","## color image일 경우 RGB 평균값으로 gray scale로 변경\n","try:\n","  image = np.mean(image, axis=2)\n","except:\n","  pass\n","## upload한 image는 흰 배경에 검은 글씨로 되어 있으므로, MNIST data와 같이 검은 배경에 흰 글씨로 변경\n","image = np.abs(255-image)\n","## MNIST와 동일하게 data preprocessing(255로 나눠줌)\n","image = image.astype(np.float32)/255.\n","## 화면에 출력하여 확인\n","plt.imshow(image, cmap='gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4d84UQwBrZ9B"},"source":["image = np.reshape(image, (1, 28, 28))\n","model.predict(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJONiB7MJKRj"},"source":["## shape을 변경하여 학습된 model에 넣고 결과 확인\n","image = np.reshape(image, (1, 28, 28))\n","print(\"Model이 예측한 값은 {} 입니다.\".format(np.argmax(model.predict(image), -1)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpHG0epAwP2N"},"source":["## TensorFlow의 기본"]},{"cell_type":"markdown","metadata":{"id":"_3Y8uosywP2O"},"source":["### Tensors(텐서)\n","\n","Tensor는 multi-dimensional array를 나타내는 말로, TensorFlow의 기본 data type입니다"]},{"cell_type":"code","metadata":{"id":"yFktD0pLwP2Q"},"source":["## Hello World\n","hello = tf.constant([3,3], dtype=tf.float32)\n","print(hello)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4LAcWLcwP2T"},"source":["## 상수형 tensor는 아래와 같이 만들 수 있습니다\n","## 출력해보면 tensor의 값과 함께, shape과 내부의 data type을 함께 볼 수 있습니다\n","x = tf.constant([[1.0, 2.0],\n","                 [3.0, 4.0]])\n","print(x)\n","print(type(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHfDt2GwwP2W"},"source":["## 아래와 같이 numpy ndarray나 python의 list도 tensor로 바꿀 수 있습니다\n","x_np = np.array([[1.0, 2.0],\n","                [3.0, 4.0]])\n","x_list = [[1.0, 2.0], \n","         [3.0, 4.0]]\n","\n","print(type(x_np))\n","print(type(x_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mq46jscXwP2Z"},"source":["x_np = tf.convert_to_tensor(x_np)\n","x_list = tf.convert_to_tensor(x_list)\n","\n","print(type(x_np))\n","print(type(x_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfbXMMMKwP2f"},"source":["## 반대로 tensor를 다음과 같이 numpy ndarray로 바꿀 수도 있습니다\n","x.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2xc4nwcwP2j"},"source":["print(type(x.numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2OAZceowP2m"},"source":["## Tensor는 numpy ndarray와 마찬가지로 dtype과 shape이라는 속성을 가지고 있습니다\n","print('dtype:', x.dtype)\n","print('shape:', x.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nALcEhXUwP2o"},"source":["## 상수형 tensor를 생성하는 방법으로 아래와 같은 방법이 많이 사용됩니다\n","print(tf.ones(shape=(2,2))*0.2)\n","print(tf.zeros(shape=(2,2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XgL-ppdwP2s"},"source":["## tensor끼리의 4칙 연산은 element-wise 연산을 기본으로 합니다\n","a = tf.ones((2,2))*2\n","b = tf.ones((2,2))*6\n","print (a.numpy())\n","print (b.numpy())\n","\n","## 덧셈\n","print (\"덧셈\")\n","print (tf.add(a, b).numpy())\n","print ((a + b).numpy())\n","\n","## 뺄셈\n","print (\"뺄셈\")\n","print (tf.subtract(b, a).numpy())\n","print ((b - a).numpy())\n","\n","## 곱셈\n","print (\"곱셈\")\n","print (tf.multiply(a, b).numpy())\n","print ((a * b).numpy())\n","\n","## 나눗셈\n","print (\"나눗셈\")\n","print (tf.divide(b, a).numpy())\n","print ((b / a).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86J7LNglTcbk"},"source":["print(tf.matmul(a, b).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5np7uzGwP2v"},"source":["## Tensor와 numpy ndarray는 많은 경우에 자동으로 호환됩니다\n","ndarray = np.ones((3, 3))\n","print(ndarray)\n","\n","## Tensor연산에 입력으로 tensor가 아닌 ndarray가 입력으로 들어갈 수 있습니다\n","print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n","tensor = tf.multiply(ndarray, 10)\n","print(tensor)\n","\n","## numpy ndarray연산에 입력으로 tensor가 들어갈 수도 있습니다\n","print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n","print(np.add(tensor, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOIUWkBgwP2y"},"source":["## Random한 상수형 tensor는 다음과 같이 만들 수 있습니다\n","## 아래는 표준정규분포로부터 상수를 생성합니다\n","\n","tf.random.normal(shape=(2,2), mean=0., stddev=1.)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BG4_Xq4XwP20"},"source":["## 아래는 균등(uniform)분포로부터 random 상수를 생성합니다\n","tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-cq6dYpwP23"},"source":["### Variables (변수)\n","[Variables](https://www.tensorflow.org/guide/variable)는 변할 수 있는 상태를 저장하는데 사용되는 특별한 텐서 입니다. \n","\n","우리는 대부분의 경우에 우리가 학습해야하는 가중치(weight, parameter)들을 variable로 생성합니다."]},{"cell_type":"code","metadata":{"id":"kEyEcL-hwP24"},"source":["## 초기값을 사용해서 Variable을 생성할 수 있습니다\n","initial_value = tf.random.normal(shape=(2, 2))\n","weight = tf.Variable(initial_value)\n","print(weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vW68p5gAwP28"},"source":["## 아래와 같이 variable을 초기화해주는 initializer들을 사용할 수도 있습니다\n","weight = tf.Variable(tf.random_normal_initializer(stddev=1.)(shape=(2,2)))\n","print(weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeZfL3dcwP2_"},"source":["## variable은 `.assign(value)`, `.assign_add(increment)`, 또는 `.assign_sub(decrement)`\n","## 와 같은 메소드를 사용해서 Variable의 값을 갱신합니다:'''\n","\n","new_value = tf.random.normal(shape=(2,2))\n","print(new_value)\n","weight.assign(new_value)\n","print(weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1V-jIlXwP3B"},"source":["added_value = tf.ones(shape=(2,2))\n","weight.assign_sub(added_value)\n","print(weight)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otxR2EWuwP3N"},"source":["### Tensor Operations"]},{"cell_type":"markdown","metadata":{"id":"CmLzr371wP3O"},"source":["#### Indexing, Slicing"]},{"cell_type":"code","metadata":{"id":"nWQWjstPwP3P"},"source":["x = tf.constant([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRJniwEiwP3S"},"source":["## indexing - indexing을 사용하면 항상 차원이 감소합니다\n","print(x[0])\n","print(x[1])\n","print(x[2])\n","print(x[0, 1])\n","print(x[1, 2])\n","print(x[2, 3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpLIGfcOwP3W"},"source":["## slicing\n","print(x[2:, 3:])\n","# print(x[:2, 1:3])\n","# print(x[1:3, 3:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBLqq7wXwP3Z"},"source":["#### Reshape"]},{"cell_type":"code","metadata":{"id":"JKObNNXpwP3a"},"source":["t = tf.constant([[[0, 1, 2], \n","                [3, 4, 5]],              \n","               [[6, 7, 8], \n","                [9, 10, 11]]])\n","print(t.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOIJ05-RwP3d"},"source":["print(tf.reshape(t, shape=[-1, 3]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bl6gMrLqwP3g"},"source":["print(tf.reshape(t, shape=[-1, 1, 3]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xwk7MP1DwP3q"},"source":["#### Reduce Mean/Sum"]},{"cell_type":"code","metadata":{"id":"opingndLwP3q"},"source":["x = tf.constant([[1., 2.],\n","                [3., 4.]])\n","\n","print(x)\n","print(tf.reduce_mean(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXrXXCnJwP3t"},"source":["print(tf.reduce_mean(x, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Si5NR-d_wP3v"},"source":["print(tf.reduce_mean(x, axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6i41fST1wP3x"},"source":["print(tf.reduce_mean(x, axis=-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_W6aJTowP3z"},"source":["여기서 축(axis)은 각 배열의 차원에 해당되는 인덱스입니다. 위의 예를 설명하면\n","\n","- x.shape 은 (2, 2) 입니다.\n","- tf.reduce_mean(x, axis=0) 은\n","- x.shape[axis]: x.shape[0] 에 대하여 연산을 하라는 의미\n","입니다.\n","\n","X.shape == (5, 3, 2) 인 경우를 생각해봅시다. 이 경우 tf.reduce_mean(X, axis=1) 의 결과값은\n","\n","1. X.shape[axis] => X.shape[1] 에 대해서 연산을 하기 때문에\n","2. tf.reduce_mean(X, axis=1).shape 은 (5, 3, 2) -> (5, 2) 가 됩니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7RfW9UeswP30"},"source":["print(tf.reduce_sum(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"53pZvjDCwP35"},"source":["print(tf.reduce_sum(x, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8siygT_LwP37"},"source":["print(tf.reduce_sum(x, axis=-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBEDt2GlwP4J"},"source":["#### Argmax"]},{"cell_type":"code","metadata":{"id":"bjy3fD4QwP4K"},"source":["x = [[3, 4, 5],\n","     [5, 4, 3]]\n","print(x)\n","print(tf.argmax(x, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUR4kli7wP4M"},"source":["print(tf.argmax(x, axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DfpSPhQwP4Q"},"source":["print(tf.argmax(x, axis=-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ySU5JA7wP4S"},"source":["#### One-hot Encoding"]},{"cell_type":"code","metadata":{"id":"la2hlpqtwP4S"},"source":["label = tf.constant([0, 1, 2, 0])\n","onehot1 = tf.one_hot(label, depth=3)\n","onehot2 = keras.utils.to_categorical(label, num_classes=3)\n","\n","print(onehot1, type(onehot1))\n","print(onehot2, type(onehot2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L0cg-32PwP4V"},"source":["#### Type Casting"]},{"cell_type":"code","metadata":{"id":"jatYT4zLwP4W"},"source":["print(tf.cast([1.8, 2.2, 3.3, 4.9], tf.int32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tICoHJuzwP4Y"},"source":["print(tf.cast([True, False, 1 == 1, 0 == 1], tf.int32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9h0d3o9FwP4a"},"source":["### tf.data를 이용하여 Dataset 만들기\n","\n","TensorFlow를 이용하여 deep learning model을 학습할 때, input data 및 label을 공급해주기 위하여 tf.data.Dataset을 이용합니다."]},{"cell_type":"code","metadata":{"id":"fZqlWSXwZArX"},"source":["a = np.arange(10)\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mbHU-rhwP4b"},"source":["## 0에서 9까지 정수가 input data라고 가정해봅시다\n","a = np.arange(10)\n","print(a)\n","\n","## dataset은 아래와 같이 만들 수 있습니다\n","ds_tensors = tf.data.Dataset.from_tensor_slices(a)\n","print(ds_tensors)\n","\n","## dataset에서 앞 5개 data를 꺼내서 확인해보겠습니다\n","#data = ds_tensors.take(5)\n","for x in ds_tensors:\n","    print (x)\n","    #model(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5AKSCPewP4c"},"source":["## dataset 내의 각 data에 함수를 적용하기 위해서는 아래와 같이 map을 사용합니다\n","## 이는 data 전처리에 많이 사용됩니다\n","## 또한 data를 섞어주는 shuffle과 batch size만큼 data를 꺼내주는 batch도 사용할 수 있습니다\n","\n","ds_tensors = ds_tensors.map(tf.square).shuffle(10).batch(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfFl2mbDwP4e"},"source":["## 실제 data를 꺼내서 사용할 때는 아래와 같이 for문에 dataset을 넣어주면 됩니다.\n","print('Elements of ds_tensors:')\n","print('='*50)\n","for _ in range(3):\n","    for x in ds_tensors:\n","        print(x)\n","    print('='*50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kRWftGYRwP4f"},"source":["### TensorFlow를 이용하여 자동미분 계산하기\n","\n","Deep learning model을 학습시키기 위해서는 gradient descent 방법을 사용하고, 이를 위해서는 gradient 즉 미분을 계산해야 합니다.\n","(Loss 를 weight로 미분)\n","\n","TensorFlow에서 자동으로 미분을 계산하는 방법을 알아보겠습니다"]},{"cell_type":"markdown","metadata":{"id":"wX4XMtkhwP4g"},"source":["TensorFlow는 자동 미분을 위한 tf.GradientTape API를 제공합니다. \n","\n","tf.GradientTape는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\"합니다. \n","\n","그 다음 TensorFlow는 후진 방식 자동 미분(reverse mode differentiation)을 사용해 테이프에 \"기록된\" 연산의 그래디언트를 계산합니다."]},{"cell_type":"code","metadata":{"id":"pdieSE-vwP4g"},"source":["## GradientTape를 열게되면, 그때부턴 tape.watch()를 통해 tensor를 확인하고, \n","## 이 tensor를 입력으로써 사용하는 미분을 자동으로 계산하는것이 가능합니다.\n","x = tf.ones((1,))*3\n","\n","with tf.GradientTape() as t:\n","    t.watch(x)\n","    y = tf.square(x)\n","\n","# 입력 텐서 x에 대한 z의 미분\n","dy_dx = t.gradient(y, x)\n","print(dy_dx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqchwXE2wP4i"},"source":["## 기본적으로 Variable들은 자동으로 watch가 적용되어 있기 때문에, 수동으로 `watch`를 해 줄 필요는 없습니다.\n","x = tf.Variable(x)\n","\n","with tf.GradientTape() as t:\n","    y = tf.square(x)\n","\n","# 입력 텐서 x에 대한 z의 미분\n","dy_dx = t.gradient(y, x)\n","print(dy_dx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmv0r54owP4j"},"source":["## End-to-End 예제: 선형 회귀\n","\n","가장 간단한 model중 하나인 선형 회귀(linear regression) 모델을 만들고 학습을 해보겠습니다"]},{"cell_type":"code","metadata":{"id":"SXdupTf0wP4j"},"source":["## Library import\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXFlHqktwP4n"},"source":["## For reproducibility\n","np.random.seed(777)\n","tf.random.set_seed(777)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4VuxaSMbUeV"},"source":["x = np.linspace(0, 1, 100, dtype=np.float32)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RooDcCXmwP4p"},"source":["## Input data 생성\n","## y = 1 * x + 2를 따르는 noisy한 data를 100개 생성함\n","\n","## inputs\n","x = np.linspace(0, 1, 100, dtype=np.float32)\n","\n","## ground truth\n","# 기울기\n","slopes = 1\n","# 절편에 noise 추가\n","intercept = np.random.normal(2, 0.2, 100).astype(np.float32)\n","\n","## outputs\n","# y = 1*x + 2\n","y = x * slopes + intercept"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QXRhyKqwP4r"},"source":["print(slopes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ya0OoxxcwP4s"},"source":["print(intercept)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JceOUCPvwP4w"},"source":["## 만들어진 data와 ground truth 확인\n","plt.scatter(x, y)\n","#plt.plot(x, x * 1 + 2., label=\"ground truth\", c=\"r\")\n","#plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5noGAYJQwP4x"},"source":["## input data type 확인\n","x.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztTwjsHtwP4z"},"source":["## label type 확인\n","y.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQ72dHTZwP42"},"source":["## input data shape 확인\n","x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUqD3RZPwP44"},"source":["## label shape 확인\n","y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCez8Fo9wP46"},"source":["## Dataset 만들기\n","dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","dataset = dataset.shuffle(buffer_size=100).batch(50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeyxcB58wP47"},"source":["## Weight와 bias 만들기\n","w = tf.Variable(.1, tf.float32)\n","b = tf.Variable(0., tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05LSrYDRwP49"},"source":["# learning rate\n","learning_rate = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"va195J4fwP4-"},"source":["## linear regression model 만들기\n","def compute_predictions(x):\n","    return x * w + b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSz-k51EwP5A"},"source":["## loss function - Mean Squared Error\n","def compute_loss(labels, predictions):\n","    return tf.reduce_mean(tf.square(labels - predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCJKNdUqwP5B"},"source":["## gradient 계산하여 gradient descent 학습법으로 weight와 bias update\n","def train_on_batch(x, y):\n","    with tf.GradientTape() as tape:\n","        predictions = compute_predictions(x)\n","        loss = compute_loss(y, predictions)\n","        dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n","    w.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6sg0Lv0wP5D"},"source":["## 20 epoch 동안 학습진행\n","loss_list, w_list, b_list = [], [], []\n","for epoch in range(20):\n","    loss = 0.\n","    for x, y in dataset:\n","        loss_ = train_on_batch(x, y)\n","        loss += loss_ / 2.\n","    print(epoch+1, \"\\t\", loss.numpy(), \"\\t\", w.numpy(), \"\\t\", b.numpy())\n","    loss_list.append(loss.numpy())\n","    w_list.append(w.numpy())\n","    b_list.append(b.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEfe5EIgwP5F"},"source":["# Training graph\n","plt.plot(loss_list, label=\"loss\")\n","plt.plot(w_list, label=\"w\")\n","plt.plot(b_list, label=\"b\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lJeui7VwP5H"},"source":["## 결과 확인\n","plt.scatter(x, y)\n","plt.plot(x, x * w_list[-1] + b_list[-1], label=\"model\", c=\"r\")\n","plt.plot(x, x * 1 + 2., label=\"ground truth\", c=\"g\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXJqRvEQwP5I"},"source":["plt.scatter(x, y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z762fkcGgj2y"},"source":[""],"execution_count":null,"outputs":[]}]}